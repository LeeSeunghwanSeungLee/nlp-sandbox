{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled3.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNFK4iEkPviDwfG01SEh61d"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"UfLlymca-v58"},"outputs":[],"source":["# 다대일 분류\n","\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchtext.legacy.data import Field\n","from torchtext.legacy import datasets, data\n","import random"]},{"cell_type":"code","source":["SEED = 5\n","random.seed(SEED)\n","torch.manual_seed(SEED)"],"metadata":{"id":"FRHDQ31g_Mud"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hyperparameter\n","BATCH_SIZE = 64\n","lr = 0.001\n","EPOCHS = 10"],"metadata":{"id":"cSVP75PS_QVx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["USE_CUDA = torch.cuda.is_available()\n","DEVICE = torch.device(\"cudo\" if USE_CUDA else \"cpu\")\n","print(DEVICE)"],"metadata":{"id":"rClceFBD_U8x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TEXT = Field(sequential=True, batch_first=True, lower=True)\n","LABEL = Field(sequential=False, batch_first=True)"],"metadata":{"id":"cZq-JdBb_cnW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 전체 데이터를 훈련 데이터와 테스트 데이터 8:2 로 분리한다\n","\n","trainset, testset = datasets.IMDB.splits(TEXT, LABEL)"],"metadata":{"id":"wDi9Jwms_vFM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('trainset의 구성 요소 출력 : {}'.format(trainset.fields))"],"metadata":{"id":"KXuI4S7lAHwp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(vars(trainset[0]))"],"metadata":{"id":"sQy-0kHwCDjf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TEXT.build_vocab(trainset, min_freq = 5)\n","LABEL.build_vocab(trainset)"],"metadata":{"id":"t4Xp1erQCGLp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size = len(TEXT.vocab)\n","n_classes = 2\n","print('단어 집합의 크기 : {}'.format(vocab_size))\n","print('클래스의 개수 : {}'.format(n_classes))"],"metadata":{"id":"MCDwhAvQCSTn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(TEXT.vocab.stoi)"],"metadata":{"id":"g0wSsC3hCXPP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(LABEL.vocab.stoi)"],"metadata":{"id":"lRTDWQhRCd7v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainset, valset = trainset.split(split_ratio=0.8)"],"metadata":{"id":"aKc3mdZAChfk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 토치텍스트는 모든 텍스트를 배치 처리하는 것을 지원하고, 단어를 인덱스 번호로 대체하는 BucketIterator를 제공합니다.\n","\n","train_iter, val_iter, test_iter = data.BucketIterator.splits(\n","        (trainset, valset, testset), batch_size=BATCH_SIZE,\n","        shuffle=True, repeat=False)"],"metadata":{"id":"7vHmCZrdCroZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('훈련 데이터의 미니 배치의 개수 : {}'.format(len(train_iter)))\n","print('테스트 데이터의 미니 배치의 개수 : {}'.format(len(test_iter)))\n","print('검증 데이터의 미니 배치의 개수 : {}'.format(len(val_iter)))"],"metadata":{"id":"LUuDozAUC0ZB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch = next(iter(train_iter)) # 첫번째 미니배치\n","print(batch.text.shape)\n","\n","batch = next(iter(train_iter)) # 두번째 미니배치\n","print(batch.text.shape)"],"metadata":{"id":"1PXCcUfyDCec"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# reset\n","\n","train_iter, val_iter, test_iter = data.BucketIterator.splits(\n","        (trainset, valset, testset), batch_size=BATCH_SIZE,\n","        shuffle=True, repeat=False)"],"metadata":{"id":"RQgCmpgmDGv7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GRU(nn.Module):\n","    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n","        super(GRU, self).__init__()\n","        self.n_layers = n_layers\n","        self.hidden_dim = hidden_dim\n","\n","        self.embed = nn.Embedding(n_vocab, embed_dim)\n","        self.dropout = nn.Dropout(dropout_p)\n","        self.gru = nn.GRU(embed_dim, self.hidden_dim,\n","                          num_layers=self.n_layers,\n","                          batch_first=True)\n","        self.out = nn.Linear(self.hidden_dim, n_classes)\n","\n","    def forward(self, x):\n","        x = self.embed(x)\n","        h_0 = self._init_state(batch_size=x.size(0)) # 첫번째 히든 스테이트를 0벡터로 초기화\n","        x, _ = self.gru(x, h_0)  # GRU의 리턴값은 (배치 크기, 시퀀스 길이, 은닉 상태의 크기)\n","        h_t = x[:,-1,:] # (배치 크기, 은닉 상태의 크기)의 텐서로 크기가 변경됨. 즉, 마지막 time-step의 은닉 상태만 가져온다.\n","        self.dropout(h_t)\n","        logit = self.out(h_t)  # (배치 크기, 은닉 상태의 크기) -> (배치 크기, 출력층의 크기)\n","        return logit\n","\n","    def _init_state(self, batch_size=1):\n","        weight = next(self.parameters()).data\n","        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()"],"metadata":{"id":"1alb--j4DKCv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = GRU(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)"],"metadata":{"id":"ZH72jL79ERhG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, optimizer, train_iter):\n","    model.train()\n","    for b, batch in enumerate(train_iter):\n","        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n","        y.data.sub_(1)  # 레이블 값을 0과 1로 변환\n","        \n","\n","        logit = model(x)\n","        loss = F.cross_entropy(logit, y)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()"],"metadata":{"id":"I0g4BiyJEX1t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, val_iter):\n","    \"\"\"evaluate model\"\"\"\n","    model.eval()\n","    corrects, total_loss = 0, 0\n","    for batch in val_iter:\n","        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n","        y.data.sub_(1) # 레이블 값을 0과 1로 변환\n","        logit = model(x)\n","        loss = F.cross_entropy(logit, y, reduction='sum')\n","        total_loss += loss.item()\n","        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n","    size = len(val_iter.dataset)\n","    avg_loss = total_loss / size\n","    avg_accuracy = 100.0 * corrects / size\n","    return avg_loss, avg_accuracy"],"metadata":{"id":"umBISeL7Egpw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_val_loss = None\n","for e in range(1, EPOCHS+1):\n","    train(model, optimizer, train_iter)\n","    val_loss, val_accuracy = evaluate(model, val_iter)\n","\n","    print(\"[Epoch: %d] val loss : %5.2f | val accuracy : %5.2f\" % (e, val_loss, val_accuracy))\n","\n","    # 검증 오차가 가장 적은 최적의 모델을 저장\n","    if not best_val_loss or val_loss < best_val_loss:\n","        if not os.path.isdir(\"snapshot\"):\n","            os.makedirs(\"snapshot\")\n","        torch.save(model.state_dict(), './snapshot/txtclassification.pt')\n","        best_val_loss = val_loss"],"metadata":{"id":"y0K9CfvjEmWK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_state_dict(torch.load('./snapshot/txtclassification.pt'))\n","test_loss, test_acc = evaluate(model, test_iter)\n","print('테스트 오차: %5.2f | 테스트 정확도: %5.2f' % (test_loss, test_acc))` `"],"metadata":{"id":"-7BwEQ-oEnhF"},"execution_count":null,"outputs":[]}]}