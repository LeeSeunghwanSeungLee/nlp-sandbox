{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HuggingFaceTokenizer.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP3mA2C7m+RpBhjgg4AuBLq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"NT06yhMPLbC1"},"outputs":[],"source":["! pip install tokenizers"]},{"cell_type":"code","source":["import pandas as pd\n","import urllib.request\n","from tokenizers import BertWordPieceTokenizer"],"metadata":{"id":"3MyEG-jlNukQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")"],"metadata":{"id":"x9pSIC-nNzkg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["naver_df = pd.read_table('ratings.txt')\n","naver_df = naver_df.dropna(how = 'any')"],"metadata":{"id":"TcR37o_rN3QC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('naver_review.txt', 'w', encoding = 'utf8') as f:\n","    f.write('\\n'.join(naver_df['document']))"],"metadata":{"id":"4j52rCEAN9n7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertWordPieceTokenizer(lowercase=False)"],"metadata":{"id":"sWVbfIevOH12"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_file = 'naver_review.txt'\n","vocab_size = 30000\n","limit_alphabet = 6000\n","min_frequency = 5\n","\n","tokenizer.train(files=data_file,\n","                vocab_size=vocab_size,\n","                limit_alphabet=limit_alphabet,\n","                min_frequency=min_frequency)"],"metadata":{"id":"c5MpMuQiOPvR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.save_model('./')"],"metadata":{"id":"kPu3ZZtdOo2g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_fwf('vocab.txt', header = None)\n","df"],"metadata":{"id":"d5Ku9sT2Otps"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded = tokenizer.encode('아 배고픈데 짜장면먹고싶다')\n","print('토큰화 결과 :',encoded.tokens)\n","print('정수 인코딩 :',encoded.ids)\n","print('디코딩 :',tokenizer.decode(encoded.ids))"],"metadata":{"id":"k7DiHXF9OxLB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded = tokenizer.encode('커피 한잔의 여유를 즐기다')\n","print('토큰화 결과 :',encoded.tokens)\n","print('정수 인코딩 :',encoded.ids)\n","print('디코딩 :',tokenizer.decode(encoded.ids))"],"metadata":{"id":"RwW2l-owO6Uk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 기타 다른 토크나이저 활용\n","\n","#### 이 외 ByteLevelBPETokenizer, CharBPETokenizer, SentencePieceBPETokenizer 등이 존재하며 선택에 따라서 사용할 수 있습니다.\n","\n","* BertWordPieceTokenizer : BERT에서 사용된 워드피스 토크나이저(WordPiece Tokenizer)\n","* CharBPETokenizer : 오리지널 BPE\n","* ByteLevelBPETokenizer : BPE의 바이트 레벨 버전\n","* SentencePieceBPETokenizer : 앞서 본 패키지 센텐스피스(SentencePiece)와 호환되는 BPE 구현체"],"metadata":{"id":"uVLCYj0APAvU"}},{"cell_type":"code","source":["from tokenizers import ByteLevelBPETokenizer, CharBPETokenizer, SentencePieceBPETokenizer"],"metadata":{"id":"a8hawQrdO9-Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"tX-11QeuPJrH"},"execution_count":null,"outputs":[]}]}