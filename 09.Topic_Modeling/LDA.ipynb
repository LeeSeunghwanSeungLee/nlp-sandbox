{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LDA.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOkFzakaUKRu2gaNWHGQzZE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"24MRKAI31b75"},"outputs":[],"source":["import pandas as pd\n","from sklearn.datasets import fetch_20newsgroups\n","import nltk\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import TruncatedSVD"]},{"cell_type":"code","source":["dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n","documents = dataset.data\n","print('샘플의 수 :',len(documents))"],"metadata":{"id":"5jLHe6Uu8rqq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["news_df = pd.DataFrame({'document':documents})\n","# 특수 문자 제거\n","news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n","# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n","news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n","# 전체 단어에 대한 소문자 변환\n","news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"],"metadata":{"id":"HHKK9Du48t8Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","# NLTK로부터 불용어를 받아온다.\n","nltk.download('stopwords')\n","\n","stop_words = stopwords.words('english')\n","tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n","tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n","# 불용어를 제거합니다."],"metadata":{"id":"deUxWsE384Es"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_doc[:5]"],"metadata":{"id":"k9cCHZKs8678"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from gensim import corpora\n","dictionary = corpora.Dictionary(tokenized_doc)\n","corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n","print(corpus[1]) # 수행된 결과에서 두번째 뉴스 출력. 첫번째 문서의 인덱스는 0"],"metadata":{"id":"d-7cG3Ip9F6R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(dictionary[66])"],"metadata":{"id":"rWeIT4OZ9PRq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(dictionary)"],"metadata":{"id":"CTyTbKH79S_L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 20개의 카테고리를 가진 뉴스로 예시를 듬\n","\n","import gensim\n","\n","NUM_TOPICS = 20 # 20개의 토픽, k=20\n","ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n","topics = ldamodel.print_topics(num_words=4)\n","for topic in topics:\n","    print(topic)"],"metadata":{"id":"gN-kUBU49YG1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(ldamodel.print_topics())"],"metadata":{"id":"yJSXUe1v9kz7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 뉴스 기사 제목 데이터를 이용한 lda"],"metadata":{"id":"W6_GJOtq-spv"}},{"cell_type":"code","source":["import pandas as pd\n","import urllib.request\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation\n","\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/19.%20Topic%20Modeling/dataset/abcnews-date-text.csv\", filename=\"abcnews-date-text.csv\")\n","\n","data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False)\n","print('뉴스 제목 개수 :',len(data))"],"metadata":{"id":"ErzFC7Pc-lPe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = data[['headline_text']]\n","text.head(5)"],"metadata":{"id":"sGeRxeMN-1wf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text['headline_text'] = text.apply(lambda row: nltk.word_tokenize(row['headline_text']), axis=1) # preprocess\n","print(text.head(5))"],"metadata":{"id":"HpNRhNTn_lYN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stop_words = stopwords.words('english') # stopword\n","text['headline_text'] = text['headline_text'].apply(lambda x: [word for word in x if word not in (stop_words)])"],"metadata":{"id":"f2YRr0l1_uCx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x]) # lemmatization\n","print(text.head(5))\n"],"metadata":{"id":"Ri3s92_1_uAS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_doc = text['headline_text'].apply(lambda x: [word for word in x if len(word) > 3]) # size boundary\n","print(tokenized_doc[:5])"],"metadata":{"id":"HkSyzndn_t9g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 역토큰화 (토큰화 작업을 되돌림)\n","detokenized_doc = []\n","for i in range(len(text)):\n","    t = ' '.join(tokenized_doc[i])\n","    detokenized_doc.append(t)\n","\n","# 다시 text['headline_text']에 재저장\n","text['headline_text'] = detokenized_doc"],"metadata":{"id":"VRKzyd6G_t7S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 상위 1,000개의 단어를 보존 \n","vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000)\n","X = vectorizer.fit_transform(text['headline_text'])\n","\n","# TF-IDF 행렬의 크기 확인\n","print('TF-IDF 행렬의 크기 :',X.shape)"],"metadata":{"id":"Tr_kSpCuAADT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lda_model = LatentDirichletAllocation(n_components=10,learning_method='online',random_state=777,max_iter=1)\n","lda_top = lda_model.fit_transform(X)"],"metadata":{"id":"EjfGQG0jAAA1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(lda_model.components_)\n","print(lda_model.components_.shape) "],"metadata":{"id":"A4S4MiMl__-j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 단어 집합. 1,000개의 단어가 저장됨.\n","terms = vectorizer.get_feature_names()\n","\n","def get_topics(components, feature_names, n=5):\n","    for idx, topic in enumerate(components):\n","        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])\n","\n","get_topics(lda_model.components_,terms)"],"metadata":{"id":"PE64WmT___8A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"XgwUeJ4A__55"},"execution_count":null,"outputs":[]}]}