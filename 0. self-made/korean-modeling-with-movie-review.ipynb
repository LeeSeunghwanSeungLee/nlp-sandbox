{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "한국어 스팀 리뷰.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLo7_wbAhkvZ"
      },
      "outputs": [],
      "source": [
        "# Colab에 Mecab 설치\n",
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab190912.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from collections import Counter\n",
        "from konlpy.tag import Mecab\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "r6_90Pglhvir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/steam.txt\", filename=\"steam.txt\")"
      ],
      "metadata": {
        "id": "zpTTCzA1i7iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_data = pd.read_table('steam.txt', names=['label', 'reviews'])\n",
        "print('전체 리뷰 개수 :',len(total_data)) # 전체 리뷰 개수 출력"
      ],
      "metadata": {
        "id": "KUKTfSACi9rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_data[:5]"
      ],
      "metadata": {
        "id": "c_rKJDi5jIS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_data['reviews'].nunique(), total_data['label'].nunique()"
      ],
      "metadata": {
        "id": "yvl_G0KdjLkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_data.drop_duplicates(subset=['reviews'], inplace=True) # reviews 열에서 중복인 내용이 있다면 중복 제거\n",
        "print('총 샘플의 수 :',len(total_data))"
      ],
      "metadata": {
        "id": "14ff7WUhjWWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(total_data.isnull().values.any())"
      ],
      "metadata": {
        "id": "an47pRiBjZMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이플 분포 확인\n",
        "\n",
        "total_data['label'].value_counts().plot(kind = 'bar')"
      ],
      "metadata": {
        "id": "qWSKmmQEj4Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(total_data.groupby('label').size().reset_index(name = 'count'))"
      ],
      "metadata": {
        "id": "VdvBKXCfkBBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 가공, 전처리"
      ],
      "metadata": {
        "id": "gHKrYfA5kVEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 한글과 공백을 제외하고 모두 제거\n",
        "\n",
        "total_data['reviews'] = total_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "total_data['reviews'].replace('', np.nan, inplace=True)\n",
        "print(total_data.isnull().sum())"
      ],
      "metadata": {
        "id": "YG5yBQ3-kXdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stopwords 정의\n",
        "\n",
        "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게', '만', '게임', '겜', '되', '음', '면']"
      ],
      "metadata": {
        "id": "XsYxFpUyqthc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mecab을 활용한 형태소 분리\n",
        "\n",
        "mecab = Mecab() \n",
        "\n",
        "total_data['tokenized'] = total_data['reviews'].apply(mecab.morphs) # 형태소 분리\n",
        "total_data['tokenized'] = total_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords]) # 불용어 제거"
      ],
      "metadata": {
        "id": "gbEyIx2vq30Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_words = np.hstack(total_data[total_data.label == 0]['tokenized'].values)\n",
        "positive_words = np.hstack(total_data[total_data.label == 1]['tokenized'].values)"
      ],
      "metadata": {
        "id": "pFPqXa51rFQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_word_count = Counter(negative_words)\n",
        "print(negative_word_count.most_common(20))"
      ],
      "metadata": {
        "id": "aGYg8urerPXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_word_count = Counter(positive_words)\n",
        "print(positive_word_count.most_common(20))"
      ],
      "metadata": {
        "id": "IUQrBnLqrYS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n",
        "text_len = total_data[total_data['label']==1]['tokenized'].map(lambda x: len(x))\n",
        "ax1.hist(text_len, color='red')\n",
        "ax1.set_title('Positive Reviews')\n",
        "ax1.set_xlabel('length of samples')\n",
        "ax1.set_ylabel('number of samples')\n",
        "print('긍정 리뷰의 평균 길이 :', np.mean(text_len))\n",
        "\n",
        "text_len = total_data[total_data['label']==0]['tokenized'].map(lambda x: len(x))\n",
        "ax2.hist(text_len, color='blue')\n",
        "ax2.set_title('Negative Reviews')\n",
        "fig.suptitle('Words in texts')\n",
        "ax2.set_xlabel('length of samples')\n",
        "ax2.set_ylabel('number of samples')\n",
        "print('부정 리뷰의 평균 길이 :', np.mean(text_len))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FyJ2FCf1rb55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bow, n-gram, tf-idf 활용을 위한 모듈 & 사이킷 런 모듈들..\n",
        "\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import gensim\n",
        "\n",
        "# import scikitplot.plotters as skplt\n",
        "\n",
        "import nltk\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import os\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "# from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "dcg0kMQxrgzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_data.head()"
      ],
      "metadata": {
        "id": "mMj9_0SfsD6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(total_data.tokenized.tolist()[0])"
      ],
      "metadata": {
        "id": "4zdb6jpksVhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Back of words\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform([doc1,doc2,doc3])\n",
        "df_bow_sklearn = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())\n",
        "df_bow_sklearn.head()"
      ],
      "metadata": {
        "id": "k5OEV06zssdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dtm\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    'you know I want your love',\n",
        "    'I like you',\n",
        "    'what should I do ',    \n",
        "]\n",
        "\n",
        "vector = CountVectorizer()\n",
        "\n",
        "# 코퍼스로부터 각 단어의 빈도수를 기록\n",
        "print(vector.fit_transform(corpus).toarray())\n",
        "\n",
        "# 각 단어와 맵핑된 인덱스 출력\n",
        "print(vector.vocabulary_)"
      ],
      "metadata": {
        "id": "DIt43tAluE4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf-idf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = [\n",
        "    'you know I want your love',\n",
        "    'I like you',\n",
        "    'what should I do ',    \n",
        "]\n",
        "\n",
        "tfidfv = TfidfVectorizer().fit(corpus)\n",
        "print(tfidfv.transform(corpus).toarray())\n",
        "print(tfidfv.vocabulary_)"
      ],
      "metadata": {
        "id": "imDvpG7QuVMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gHiv841EuY9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[링크](https://wikidocs.net/94748)"
      ],
      "metadata": {
        "id": "t7UJjb-_uufR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Mxxtg8xhuv4Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}