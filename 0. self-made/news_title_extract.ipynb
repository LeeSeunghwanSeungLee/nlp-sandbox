{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0.news-title-extract.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# news-title-extract 테스트 진행하기\n",
        "\n",
        "### BoW\n",
        "\n",
        "### N-gram\n",
        "\n",
        "### TF-IDF"
      ],
      "metadata": {
        "id": "9XpJdeoAxqsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import Sastrawi\n",
        "except ModuleNotFoundError:\n",
        "    ! pip install PySastrawi\n",
        "finally:\n",
        "    from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "! pip install feedparser\n",
        "! pip install goose3\n",
        "! pip install gensim"
      ],
      "metadata": {
        "id": "fwbrYU8oHuHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import datetime\n",
        "import feedparser\n",
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from goose3 import Goose\n",
        "from goose3.text import StopWordsKorean\n",
        "from gensim.summarization.summarizer import summarize\n",
        "import warnings\n",
        "\n",
        "# To Ignore warning sign\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# Class :\n",
        "class newsScrap():\n",
        "    \"\"\"Main Class related News Crawl\"\"\"\n",
        "\n",
        "    def __init__(self): \n",
        "        print(\"Constructor\")\n",
        "        self._title = []\n",
        "    def __del__(self): \n",
        "        print(\"Garbage Collection\")\n",
        "\n",
        "    \"\"\"Get Webpage response method\n",
        "        @param keyword : keyword to look for news\n",
        "        @param day : how many days ago from today\n",
        "        @param country : 2 types [\"ko\" || \"en\"]\n",
        "        @param news_room : RSS feed list\n",
        "    \"\"\"\n",
        "    def eccess(self, news_room):\n",
        "        print(\"Crawl start\")\n",
        "        URL = news_room # you need to override this method\n",
        "        \n",
        "        res = requests.get(URL)\n",
        "        if res.status_code == 200:\n",
        "            datas = feedparser.parse(res.text).entries ## what is entries?\n",
        "            \n",
        "            for data in datas:\n",
        "                self._title.append(data.title)\n",
        "\n",
        "        else:\n",
        "            print(\"No response\")\n",
        "\n",
        "\n",
        "    \"\"\"Set data frame & change format & save (can override)\"\"\"\n",
        "    def setDataFrame(self):\n",
        "        raw_data = {'title' : self._title}\n",
        "        res = pd.DataFrame(raw_data)\n",
        "        file_name = \"./result.csv\"\n",
        "        if os.path.isfile(file_name):\n",
        "            os.remove(file_name)\n",
        "        res.to_csv(file_name)\n",
        "\n",
        "\n",
        "\n",
        "class googleScrap(newsScrap):\n",
        "    \"\"\"Extend NewsScrap class to use google news feed\n",
        "    @param _time : the time news writed\n",
        "    @param _link : news link\n",
        "    @param _summary : news context\n",
        "    @param _source : news source (ex. chosun, jungang...)\n",
        "    @param _keyword : title keyword(we are looking for)\n",
        "    @param _dataFrame : dataframe for changing format (.html, .csv...)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        newsScrap.__init__(self)\n",
        "        self._time = []\n",
        "        self._link = []\n",
        "        self._summary = []\n",
        "        self._source = []\n",
        "        self._keyword = []\n",
        "        self._dataFrame = None\n",
        "        self._footNote = {}\n",
        "\n",
        "    def eccess(self, keyword, day, country = 'ko'): # Google News Feed parsing method\n",
        "\n",
        "        print ('Google News Cron Start: ' + datetime.datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
        "        URL = 'https://news.google.com/rss/search?q={}+when:{}d'.format(keyword, day)\n",
        "        \n",
        "        if country == 'en':\n",
        "            URL += '&hl=en-NG&gl=NG&ceid=NG:en'\n",
        "        elif country == 'ko':\n",
        "            URL += '&hl=ko&gl=KR&ceid=KR:ko'\n",
        "\n",
        "        res = requests.get(URL)\n",
        "        if res.status_code == 200:\n",
        "            datas = feedparser.parse(res.text).entries\n",
        "            \n",
        "            for data in datas:\n",
        "                self._title.append(data.title)\n",
        "                self._time.append(data.published)\n",
        "                self._source.append(data.source.title)\n",
        "                self._keyword.append(keyword)   \n",
        "                self._link.append(data.link)\n",
        "                # try: # merge with company version\n",
        "                #     g = Goose({'stopwords_class':StopWordsKorean})\n",
        "                #     article = g.extract(url=URL)\n",
        "                #     self._summary.append(article.cleaned_text[:500])\n",
        "                #     # self._summary.append(article.meta_description)\n",
        "                #     # self._summary.append(summarize(article.cleaned_text[:1500]))\n",
        "\n",
        "                # except:\n",
        "                #     self._summary.append(data.title)\n",
        "                #     pass\n",
        "                                 \n",
        "        else:\n",
        "            print ('Google Search Error!!')\n",
        "\n",
        "    def addFootNote(self, keywords_li, country=\"kr\"):\n",
        "        for keywords in keywords_li:\n",
        "            foot_link = \"https://news.google.com/search?q={}\".format(keywords)\n",
        "        if country == 'en':\n",
        "            foot_link += '&hl=en-NG&gl=NG&ceid=NG:en'\n",
        "        elif country == 'ko':\n",
        "            foot_link += '&hl=ko&gl=KR&ceid=KR:ko'\n",
        "        self._footNote.update({keywords : foot_link})\n",
        "       \n",
        "    def setDataFrame(self):\n",
        "        raw_data = {\n",
        "            'time' : self._time,\n",
        "            'title' : self._title,\n",
        "            'link' : self._link,\n",
        "            'source' : self._source,\n",
        "            'keyword' : self._keyword\n",
        "        }\n",
        "        self._dataFrame = pd.DataFrame(raw_data)\n",
        "    \n",
        "    def createCSV(self, file_name):\n",
        "        file = './' + file_name + '.csv'\n",
        "        if os.path.isfile(file):\n",
        "            os.remove(file)\n",
        "        self._dataFrame.to_csv(file, encoding='utf-8-sig')\n",
        "\n",
        "    def createHTML(self, file_name):\n",
        "        file = './' + file_name + '.html'\n",
        "        if os.path.isfile(file):\n",
        "            os.remove(file)\n",
        "        self._dataFrame.to_html(file, encoding='utf-8-sig') # use (escape=False) if you want to make URL tag in html\n",
        "\n",
        "    def appendFootNode(self, file_name):\n",
        "        # 마크업 링크를 만든다\n",
        "        markup = \"\"\n",
        "        # ./{file_name}.html에 추가로 입력한다.\n",
        "        with open(\"./{}.html\".format(file_name), \"a\") as file:\n",
        "            file.write(markup)\n",
        "            file.close()\n",
        "        \n",
        "# if __name__ == \"__main__\":\n",
        "#     today = googleScrap()\n",
        "#     today.eccess('smartfactory', 1)\n",
        "#     today.setDataFrame()\n",
        "#     today.createHTML('result')\n",
        "#     del today"
      ],
      "metadata": {
        "id": "bzJ40ePPLZni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = ['covid', 'smartfactory', 'politics', 'korea', 'samsung', 'hyundai', 'LG', 'Apple', 'seoul', 'bts', 'stackoverflow', 'microsoft', 'netflix'\n",
        "            'disney', 'naver', 'stock', 'heungmin', 'manchester', 'kaist', 'mcu', 'doctor strange', 'kakao', 'tesla', 'musk', 'nvidia']\n",
        "\n",
        "\n",
        "today = googleScrap()\n",
        "for  keyword in keywords:\n",
        "    today.eccess(keyword, 100, 'en')\n",
        "today.setDataFrame()\n",
        "today.createCSV('news_feed')\n",
        "del today"
      ],
      "metadata": {
        "id": "5v92C_C6L2Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nnJ0G1LqxqJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('news_feed.csv')\n",
        "print(df.head())\n",
        "print('dataframe length : {}'.format(len(df)))"
      ],
      "metadata": {
        "id": "F8xg1bPpIlyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['word_count'] = df['title'].apply(lambda x: len(str(x).split(' '))) # 한글의 경우 형태소를 분리가 필요 Mecab ...\n",
        "df[['title', 'word_count']].head() # 통계자료 확인"
      ],
      "metadata": {
        "id": "oHQ5HsGjNmKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.hist(column = 'word_count', bins=20)\n",
        "df.word_count.describe()"
      ],
      "metadata": {
        "id": "X4MWMPoHN7E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Most Common and Uncommon Words per Category\n",
        "\n",
        "def get_top_words(df, n=10, column='title'):\n",
        "    top_words = pd.Series(' '.join(df[column]).split()).value_counts()[:n]\n",
        "    print(f'Top words {df.category.unique()[0]} category: ', end='') # df 에 category 를 추가할 필요가 있음\n",
        "    for index, value in top_words.items():\n",
        "        print(f'{index} ({value}), ', end='')\n",
        "    print('\\n')\n",
        "\n",
        "for category in df.category.unique():\n",
        "    get_top_words(df[df.category == category])"
      ],
      "metadata": {
        "id": "Qg8Kj3lSI1yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a regex\n",
        "\n",
        "import re\n",
        "def preprocess(text):\n",
        "    # remove punctuation\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    # convert to lower case\n",
        "    text = text.lower()\n",
        "    # remove special char and digit\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "    # remove stop words\n",
        "    stopword = StopWordRemoverFactory().create_stop_word_remover() \n",
        "    text = stopword.remove(text)\n",
        "    # stemming\n",
        "    stemmer = StemmerFactory().create_stemmer()\n",
        "    text = stemmer.stem(text)\n",
        "    return text\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r7aTQReEPW43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check time \n",
        "\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "for i in range(0, len(df)):\n",
        "    if (i%500) == 0:\n",
        "        mid_time = time.time()\n",
        "        print(f'Processing #{i} data, {(mid_time - start_time):.2f} seconds elapsed.')\n",
        "\n",
        "    df.at[i, 'corpus'] = preprocess(df['title'][i])\n",
        "\n",
        "end_time = time.time()\n",
        "print(f'End processing {len(df)} data for {(end_time-start_time):.2f} seconds.')\n",
        "df.head(15)"
      ],
      "metadata": {
        "id": "htHLA3WyPd-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting Topic using tf-idf\n",
        "\n",
        "## example code\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv=CountVectorizer(max_df=0.8, max_features=10000, ngram_range=(1,2))\n",
        "X=cv.fit_transform(df['corpus'])"
      ],
      "metadata": {
        "id": "AZFYdopEPhTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visuallize top word\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#Most frequently occuring words\n",
        "def get_top_n_words(corpus, n=None):\n",
        "    vec = CountVectorizer().fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
        "                   vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
        "                       reverse=True)\n",
        "    return words_freq[:n]\n",
        "#Convert most freq words to dataframe for plotting bar plot\n",
        "top_words = get_top_n_words(df['corpus'], n=10)\n",
        "top_df = pd.DataFrame(top_words)\n",
        "top_df.columns=[\"Word\", \"Freq\"]\n",
        "# #Barplot of most freq words\n",
        "import seaborn as sns\n",
        "sns.set(rc={'figure.figsize':(15,8)})\n",
        "g = sns.barplot(x=\"Freq\", y=\"Word\", data=top_df)"
      ],
      "metadata": {
        "id": "cSvwEZcVQEGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Most frequently occuring Bi-grams\n",
        "\n",
        "def get_top_n2_words(corpus, n=None):\n",
        "    vec1 = CountVectorizer(ngram_range=(5,5),  \n",
        "            max_features=2000).fit(corpus)\n",
        "    bag_of_words = vec1.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
        "                  vec1.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
        "                reverse=True)\n",
        "    return words_freq[:n]\n",
        "top2_words = get_top_n2_words(df['corpus'], n=10)\n",
        "top2_df = pd.DataFrame(top2_words)\n",
        "top2_df.columns=[\"Bi-gram\", \"Freq\"]\n",
        "#Barplot of most freq Bi-grams\n",
        "import seaborn as sns\n",
        "sns.set(rc={'figure.figsize':(13,8)})\n",
        "h=sns.barplot(x=\"Freq\", y=\"Bi-gram\", data=top2_df)"
      ],
      "metadata": {
        "id": "RHBuNTQ-QIP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TF-IDF Vector using vector of wordcount\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "tfidf_transformer.fit(X)"
      ],
      "metadata": {
        "id": "ac7C6VPtQMf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Extracting Topic from Random Title\n",
        "feature_names=cv.get_feature_names()\n",
        "b\n",
        "#Function for sorting tf_idf in descending order\n",
        "from scipy.sparse import coo_matrix\n",
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        " \n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
        "    \n",
        "    #use only topn items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        " \n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "    \n",
        "    # word index and corresponding tf-idf score\n",
        "    for idx, score in sorted_items:\n",
        "        \n",
        "        #keep track of feature name and its corresponding score\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        " \n",
        "    #create a tuples of feature,score\n",
        "    #results = zip(feature_vals,score_vals)\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "    \n",
        "    return results\n",
        "#sort the tf-idf vectors by descending order of scores\n",
        "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "#extract only the top n; n here is 10\n",
        "keywords=extract_topn_from_vector(feature_names,sorted_items,5)\n",
        " \n",
        "# now print the results\n",
        "print(\"\\nAbstract:\")\n",
        "print(doc)\n",
        "print(\"\\nKeywords:\")\n",
        "for k in keywords:\n",
        "    print(k,keywords[k])"
      ],
      "metadata": {
        "id": "-7hfxOFVQRt1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}