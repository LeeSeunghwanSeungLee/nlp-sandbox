{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LinearRegression.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOsLhLMfE9xtyFVLllHQRz6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"niDgiaU734Bw"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"]},{"cell_type":"code","source":["torch.manual_seed(1) # 현재 실습하고 있는 파이썬 코드를 재실행 하더라도, 같은 결과가 나오도록 랜덤 시드넘버를 설정한다"],"metadata":{"id":"Y9s3byaC4uLp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train = torch.FloatTensor([[1], [2], [3], [4], [5], [6]])\n","y_train = torch.FloatTensor([[2], [4], [6], [8], [10], [12]])"],"metadata":{"id":"uoj_nH7Z4zJo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["W = torch.zeros(1, requires_grad=True) \n","print(W)"],"metadata":{"id":"FsASEGFn45aw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["b = torch.zeros(1, requires_grad = True)\n","print(b)"],"metadata":{"id":"p4Au9OBp4-BN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hypo = x_train * W + b\n","print(hypo)"],"metadata":{"id":"9MFABVq-5E3K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cost = torch.mean((hypo - y_train) ** 2) \n","print(cost)"],"metadata":{"id":"V5L0YsJ35IGu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = optim.SGD([W, b], lr=0.01)"],"metadata":{"id":"4PxZaNoE5LXn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# gradient를 0으로 초기화\n","optimizer.zero_grad() \n","# 비용 함수를 미분하여 gradient 계산\n","cost.backward() \n","# W와 b를 업데이트\n","optimizer.step() "],"metadata":{"id":"J1xt81FZ5Oc2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 전체 로직\n","\n","W = torch.zeros(1, requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=0.01)\n","\n","nb_epochs = 2000 # 원하는만큼 경사 하강법을 반복\n","for epoch in range(nb_epochs + 1):\n","\n","    # H(x) 계산\n","    hypothesis = x_train * W + b\n","\n","    # cost 계산\n","    cost = torch.mean((hypothesis - y_train) ** 2)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    # 100번마다 로그 출력\n","    if epoch % 100 == 0:\n","        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n","            epoch, nb_epochs, W.item(), b.item(), cost.item()\n","        ))"],"metadata":{"id":"hCjkWWhD5TI4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# zero_grad 가 왜 필요할까?\n","\n","import torch\n","w = torch.tensor(2.0, requires_grad=True)\n","\n","nb_epochs = 20\n","for epoch in range(nb_epochs + 1):\n","\n","  cost = 2*w\n","\n","  cost.backward()\n","  print('수식을 w로 미분한 값 : {}'.format(w.grad))"],"metadata":{"id":"4z-4Mnrx5goA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# auto grad 의 역할은 무엇일까?\n","\n","w = torch.tensor(2.0, requires_grad = True)\n","\n","y = w**2\n","z = 2 * y + 19\n","\n","z.backward()\n","\n","print(w.grad) # 미분한결과가 나온다는 것을 확인할 수 있다."],"metadata":{"id":"nOHFX__05yuk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Multivariable Linear Regression 실습\n","\n","# 훈련 데이터\n","x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n","x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n","x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n","y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n","\n","# 가중치 w와 편향 b 초기화\n","w1 = torch.zeros(1, requires_grad=True)\n","w2 = torch.zeros(1, requires_grad=True)\n","w3 = torch.zeros(1, requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","\n","# optimizer 설정\n","optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n","\n","nb_epochs = 1000\n","for epoch in range(nb_epochs + 1):\n","\n","    # H(x) 계산\n","    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n","\n","    # cost 계산\n","    cost = torch.mean((hypothesis - y_train) ** 2)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    # 100번마다 로그 출력\n","    if epoch % 100 == 0:\n","        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n","            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\n","        ))"],"metadata":{"id":"ncyBdlYm6HlC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# input data makes use of Vector\n","\n","x_train  =  torch.FloatTensor([[73,  80,  75], \n","                               [93,  88,  93], \n","                               [89,  91,  80], \n","                               [96,  98,  100],   \n","                               [73,  66,  70]])  \n","y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n","\n","print(x_train.shape)\n","print(y_train.shape)\n","\n","W = torch.zeros((3, 1), requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=1e-5)\n","\n","nb_epochs = 20\n","for epoch in range(nb_epochs + 1):\n","\n","    # H(x) 계산\n","    # 편향 b는 브로드 캐스팅되어 각 샘플에 더해집니다.\n","    hypothesis = x_train.matmul(W) + b\n","\n","    # cost 계산\n","    cost = torch.mean((hypothesis - y_train) ** 2)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n","        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n","    ))"],"metadata":{"id":"aKKRja-x7HfL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# nn.Module 로 구현하는 self made model\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","torch.manual_seed(3)\n","\n","# 데이터\n","x_train = torch.FloatTensor([[1], [2], [3]])\n","y_train = torch.FloatTensor([[2], [4], [6]])\n","\n","# 모델을 선언 및 초기화. 단순 선형 회귀이므로 input_dim=1, output_dim=1.\n","model = nn.Linear(1,1)\n","print(list(model.parameters()))\n","\n","# optimizer 설정. 경사 하강법 SGD를 사용하고 learning rate를 의미하는 lr은 0.01\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01) \n","\n","# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n","nb_epochs = 2000\n","for epoch in range(nb_epochs+1):\n","\n","    # H(x) 계산\n","    prediction = model(x_train)\n","\n","    # cost 계산\n","    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n","\n","    # cost로 H(x) 개선하는 부분\n","    # gradient를 0으로 초기화\n","    optimizer.zero_grad()\n","    # 비용 함수를 미분하여 gradient 계산\n","    cost.backward() # backward 연산\n","    # W와 b를 업데이트\n","    optimizer.step()\n","\n","    if epoch % 100 == 0:\n","    # 100번마다 로그 출력\n","      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","          epoch, nb_epochs, cost.item()\n","      ))\n","\n","\n","# 학습한 모델로 테스트 진행\n","new_var =  torch.FloatTensor([[4.0]]) \n","# 입력한 값 4에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n","pred_y = model(new_var) # forward 연산\n","# y = 2x 이므로 입력이 4라면 y가 8에 가까운 값이 나와야 제대로 학습이 된 것\n","print(\"훈련 후 입력이 4일 때의 예측값 :\", pred_y.item()) \n","\n","# 최종 모델 파라미터\n","print(list(model.parameters()))"],"metadata":{"id":"Roaog2-O7oWg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# torch 모듈 상속하여 클래스로 모델 구현하기\n","\n","class LinearRegressionModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.linear = nn.Linear(1, 1)\n","\n","    def forward(self, x):\n","        return self.linear(x)\n","\n","model = LinearRegressionModel()\n","\n","class MultivariateLinearRegressionModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.linear = nn.Linear(3, 1) # 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n","\n","    def forward(self, x):\n","        return self.linear(x)\n","\n","multiModel = MultivariateLinearRegressionModel()"],"metadata":{"id":"pS1XF-2C8Ose"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터\n","x_train = torch.FloatTensor([[73, 80, 75],\n","                             [93, 88, 93],\n","                             [89, 91, 90],\n","                             [96, 98, 100],\n","                             [73, 66, 70]])\n","y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n","\n","optimizer = torch.optim.SGD(multiModel.parameters(), lr = 1e-5)\n","\n","nb_epochs = 2000\n","for epoch in range(nb_epochs+1):\n","\n","    # H(x) 계산\n","    prediction = multiModel(x_train)\n","    # model(x_train)은 model.forward(x_train)와 동일함.\n","\n","    # cost 계산\n","    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n","\n","    # cost로 H(x) 개선하는 부분\n","    # gradient를 0으로 초기화\n","    optimizer.zero_grad()\n","    # 비용 함수를 미분하여 gradient 계산\n","    cost.backward()\n","    # W와 b를 업데이트\n","    optimizer.step()\n","\n","    if epoch % 100 == 0:\n","    # 100번마다 로그 출력\n","      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","          epoch, nb_epochs, cost.item()\n","      ))"],"metadata":{"id":"RuYSnFHZ8x-w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# mini batch example\n","\n","from torch.utils.data import TensorDataset # 텐서데이터셋\n","from torch.utils.data import DataLoader # 데이터로더\n","\n","x_train  =  torch.FloatTensor([[73,  80,  75], \n","                               [93,  88,  93], \n","                               [89,  91,  90], \n","                               [96,  98,  100],   \n","                               [73,  66,  70]])  \n","y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n","\n","dataset = TensorDataset(x_train, y_train)\n","\n","dataloader = DataLoader(dataset, batch_size = 2, shuffle = True)\n","\n","model = nn.Linear(3, 1)\n","optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)\n","\n","nb_epochs = 25\n","\n","for epoch in range(nb_epochs + 1):\n","  for batch_idx, (x_train, y_train) in enumerate(dataloader):\n","    # print(batch_idx)\n","    # print(samples)\n","    # x_train, y_train = samples\n","    # H(x) 계산\n","    prediction = model(x_train)\n","\n","    # cost 계산\n","    cost = F.mse_loss(prediction, y_train)\n","\n","    # cost로 H(x) 계산\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n","        epoch, nb_epochs, batch_idx+1, len(dataloader),\n","        cost.item()\n","        ))"],"metadata":{"id":"K1OMFRfl88zo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 임의의 입력 [73, 80, 75]를 선언\n","new_var =  torch.FloatTensor([[73, 80, 75]]) \n","# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n","pred_y = model(new_var) \n","print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y.item()) "],"metadata":{"id":"U-llGtme-k40"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Custom dataset\n","\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self):\n","        \"\"\"데이터셋의 전처리를 해주는 부분\"\"\"\n","\n","    def __len__(self):\n","        \"\"\"데이터셋의 길이. 즉, 총 샘플의 수를 알려주는 부분\"\"\"\n","\n","    def __getitem__(self, idx):\n","        \"\"\"데이터셋에서 idx 인덱스 해당하는 샘플을 리턴하는 부분\"\"\""],"metadata":{"id":"1-9YW4q5_BaU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 예제\n","\n","class CustomDataset(torch.utils.data.Dataset): \n","  def __init__(self):\n","    self.x_data = [[73, 80, 75],\n","                   [93, 88, 93],\n","                   [89, 91, 90],\n","                   [96, 98, 100],\n","                   [73, 66, 70]]\n","    self.y_data = [[152], [185], [180], [196], [142]]\n","\n","  # 총 데이터의 개수를 리턴\n","  def __len__(self): \n","    return len(self.x_data)\n","\n","  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n","  def __getitem__(self, idx): \n","    x = torch.FloatTensor(self.x_data[idx])\n","    y = torch.FloatTensor(self.y_data[idx])\n","    return x, y\n","\n","dataset = CustomDataset()\n","dataloader = DataLoader(dataset, batch_size = 2, shuffle = True)\n","\n","model = torch.nn.Linear(3, 1)\n","optimizer = torch.optim.SGD(model.parameters(), lr = 1e-6)\n","\n","nb_epochs = 200\n","for epoch in range(nb_epochs + 1):\n","  for batch_idx, samples in enumerate(dataloader):\n","    # print(batch_idx)\n","    # print(samples)\n","    x_train, y_train = samples\n","    # H(x) 계산\n","    prediction = model(x_train)\n","\n","    # cost 계산\n","    cost = F.mse_loss(prediction, y_train)\n","\n","    # cost로 H(x) 계산\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n","        epoch, nb_epochs, batch_idx+1, len(dataloader),\n","        cost.item()\n","        ))"],"metadata":{"id":"J8vztdz-AJb8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 임의의 입력 [73, 80, 75]를 선언\n","new_var =  torch.FloatTensor([[73, 80, 75]]) \n","# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n","pred_y = model(new_var) \n","print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y.item()) "],"metadata":{"id":"1sk5S3RNAT4y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"_-8wh-I-Ar7V"},"execution_count":null,"outputs":[]}]}